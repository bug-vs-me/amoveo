the checkpoint always seems corrupted when we try to sync it.
It seems like the corruption must be happening between when it gets packed and unpacked. because one_root_clean has more checks now, and they are passing on the server in germany.
trees2:scan_verkle works for the top block too.

trying to get checkpoints to sync two hops.
currently experimenting with changing the trees pointer after syncing the checkpoint.
* maybe checkpoints are corrupted if they are made while syncing.



syncing the checkpoint works.
second level syncing partially fails. it isn't cleaning the dead space from the verkle tree.



when making checkpoints normally it works, if we try to force a checkpoint immediately after syncing it is failing.





almost got the germany server to sync.
can't pull checkpoints from the server, they are corrupted.
the root doesn't pass the stem_verkle:check_root_integrity/1.





if we just turn on a node and try to sync, it is failing to copy the cleaner stuff over. if we turn the node off and on at line 459 of page checkpoint.erl, then this problem goes away.
before we do trees2:one_root_clean/2
probably some things need to get saved to the hard disk first.




ets tables store their names internally.
you can't load a second table if you already have a table with that name.
you can rename tables that are loaded.
this is preventing us from copying all the *_bits.db files.
So, for each of the *_bits gen server in the cleaner verkle database, we should loop over every bit in the database, and duplicate those bits to the main verkle database.


remove unnecessary print statements in dump.



trying to get the checkpoint to shrink.
when we load the new bits file into the amoveo, it fails at the ets:file2tab(File) step.
after that, the file ends up corrupted in a weird way. it is like it both exists and does not. seems like some thread is creating and destroying it rapidly.





trying to get the checkpoint to load.
we get the cleaner db working, but it isn't clear we are saving everything to the hard disk correctly.
if we do dump:reload(accounts_cleaner), it seems to work correctly.
but when we try dump:reload(accounts_dump), it leaves it in a broken state. bits:top(accounts_dump) timesout.

When we put data into the cleaner, we can tell the bits module to restart from empty.
when we copy back to the main db, the bits module needs to restart by reading data from the file. This seems to be where it is breaking.
it seems to get caught in an infinite loop.





if loading a checkpoint saves 2 blocks, why do we have only one after?



trying to start with a checkpoint, and do trees2:one_root_clean/2
looks like block:tree is stuck on 4. should update to new pointer after cleaning somehow.
- but, the other block is storing a tuple there. Where does the pointer get stored??




we got a second verkle tree working, and we can use it to clean all our database except for what can be proved from a single root.
trees2:one_root_clean/2
we should modify it for creating checkpoints, so the checkpoint is smaller.
we should use it when syncing checkpoints, so we don't put anything extra in the database.



tree_data:garbage is only for merkle. we should use tree:garbage to make the verkle analogue.
we need to test prune_verkle to see if it even works. we need to write new better tests.

when we sync forwards, it seems to do it in jumps. why isn't it one block at a time?



light node needs to handle orphaned blocks somehow.





add employment contracts.
It is a pretty simple mechanism. The database object is like:
{job_id 32_bytes, worker_pub 32_bytes,
 boss_pub 32 bytes, price veos,
 tax_rate 8 bytes,
 balance veos, time block_height}

Every time there is a tx, we update the job.
we update time.
we pay the tax linearly until the balance hits it's constraint, after that point the balance and price drop exponentially per block. Loss in balance is for paying taxes.


tax_rate/balance/price constraint: We must always have enough balance left over to keep paying the current tax for a week.
balance >= tax_rate_per_block * price * 1000.
If this inequality stops being true, then the price gets reset to:
price = balance / (tax_rate_per_block * 1000)
This acts as an auction.

balance_n = tax_rate_per_block * price_n * 1000.
balance_n+1 = balance_n - (tax_rate_per_block * price).
price_n+1 = balance_n+1 / (tax_rate_per_block * 1000)
          //substitute for balance_n+1
  = (balance_n - (tax_rate_per_block * price_n)) / (tax_rate_per_block * 1000)
          //substitute for balance_n
  = ((tax_rate_per_block * price_n * 1000) - (tax_rate_per_block * price_n)) / (tax_rate_per_block * 1000)
  = (tax_rate_per_block * price_n * 999) / (tax_rate_per_block * 1000)
  = (price_n) * (999/1000)


and the transaction types are:
* job_create - {worker, tax_rate, initial_balance} (signed by worker)
* job_price - {job_id, new_price} (signed by boss) 
* job_buy - {job_id, balance} (signed by who will be the new boss) old balance goes to old owner, balance is the new balance.
* job_deposit {job_id, amount} (signed by boss) to send or withdraw money from a job.
* job_tax - {job_id} (signed by both)



The tax is paid from the job's balance.
like 90% of the tax goes to the worker, and 10% is burned.

If you don't deposit veo to pay the tax, eventually the balance gets low enough, and then the price starts dropping exponentially so that it never runs out of money. To act as a continuous auction.
If you withdraw almost all the veo, then the price may be forced downwards, to accomodate this auction curve.
At a given balance, the contract has a maximum price.


links related to making maps in js.
https://help.openstreetmap.org/questions/7019/how-to-put-a-pin-at-the-map
https://leafletjs.com/
https://openlayers.org/






the new_oracle.html page is working off the old version where one scalar is 10 binary. we need to look into how much of it needs to be updated.

in lookup.html it is trying to look up governance values that no longer exist.
instead, we should just display a chart of basic blockchain info, like the block reward and block time.

in checkpoint.erl we turned off trie:clean_ets because it was causing failure to sync with the checkpoint.
We probably need to do clean_ets to prevent a memory leak, so we should try to figure out what is going wrong and fix it.


peers_heights:peers_heights is failing if some of the peers get shut down.
spawn threads to connect to each peer.






merge the rest of this todo list with the one in amoveo-docs. start working on harberger


checkpoint:verify_blocks/4 needs to be parallelized somehow. We are syncing in reverse so slow.


ext_handler:many_headers should be cached. our last attempt failed, so we should try again.
maybe the better strategy is writing headers consecutively in a file, so we can read contiguous sections of the file to send.




After update 52 activates
* for blocks earlier than 52, just don't bother calculating the merkle root of the new consensus state. (so don't do check2. just check0 and check3)
* update block:trees_hash_maker/5 for this.



Needed for Fork 52
==============

in the verkle code, it looks like tree:clean_ets isn't working. I made a test in test_verkle:clean_ets_test/0, but that test isn't passing yet either.

Write the new checkpoint code for calculating the Roots in checkpoint:sync/3

make checkpoints work even if the checkpoint was built after fork 52.
we have a test in tests/checkpoint.py

checkpoint:verify_blocks/4 should be tested for the verkle case.

new version of ubuntu.

===================




in tx explorer, it doesn't say who sent the tx.

in explorer.html you cant look up info about an account.
maybe just link to the working page instead: http://159.89.87.58:8080/explorers/account_explorer.html?pubkey=BCjdlkTKyFh7BBx4grLUGFJCedmzo4e0XT1KJtbSwq5vCJHrPltHATB+maZ+Pncjnfvt9CsCcI9Rn1vO+fPLIV4=

we need to get the tests to pass.
when we write to the ram version of the consensus state, it needs to be explicit if we are making a new slot, or editing an existing slot. a lot of places in the code will be changed for this.
* the contract_use_tx isn't working. We can't create the unhashed key from the tx, so proofs.erl can't process the tx.


looks like we need to store things in the csc by the unhashed version of the ids, because that is the only way we can look up things from both the merkle and verkle trees.


we can't  mine blocks, because the same account is being stored in 2 different locations in the merkle tree.



in the process of updating the dictionary to store the new format.
governance values are still in the old format.



When the full node is processing a block, it verifies a proof of some consensus state. It stores that slice of the consensus state in a dictionary structure while processing the txs. 

Previously, the keys for this dictionary were the same as the keys used to store in the m/verkle tree. A unique identifier for each thing being stored. When we store an empty value, it looks like {(256-bit identifier):0}

But with the new kind of verkle trees, the way proofs get compressed, if we prove that a sub-branch of the tree is empty, that can be proving that multiple values in the dictionary should be empty.

This started being corrected by storing more info in the key. Like, info that we could need about the value, so that we could generate a value that is compatible with that key. like {({key, info1, info2}):0}.
This was a mistake because in some transactions wer are looking up things by ID, and we don't know thos hidden values.
instead, we should store the default values for a newly created element in the tree. {(256-bit identifier):{account, pub="", balance=0}}, and we need to also store a flag for if it is empty.

the new data structure is in csc.erl




working on test_txs:test(36).
* a tx is blocked during the no-counterfeit check. the problem is that the dictionary is storing the info twice, because there are different formats that the key can take. it is doing the contract twice.


for the testnet we should use fast proofs instead of short ones. to make it faster.


make sure that governance oracles are disabled.


we need to implement the rest of trees2:hash_key/2


in accounts dict_empty, we need to support 2 different formats for the empty slots.
we should make similar code for other trees in all the cases where we need to prove that it is empty. update the txs to support the new format.

in test_txs:test(1), we can create a create_account_tx, but then we fail to make a spend tx in the same block because it thinks our nonce hasn't been updated. I guess the create_account_tx part isn't updating the correct slot in the dictionary.


tester:test() is failing on the test for oracles. it fails to create an account, even though the other test did make an account.

we probably need to rethink how the tx_pool works, because now we generate all the proofs at the end.


we need to rethink how syncing works to be compatible with verkle

really weird how in tx_pool_feeder:absorb_internal2 we are calculating both X and X2. Seems like it must be an error somehow.


batches of blocks might need to work differently. We should test this to make sure it is working.


try copying the current consensus state over to the verkle tree.



remove code that falsely implies the verkle tree has the ability to delete elements.


for every tree, we need to update dict_get to understand the new serialization.
oracle, matched, unmatched, sub_acc, contract, trade, market, receipt.



not calculating the market cap right with this new database format.


in block.erl, line 824.
problem with loading the dict from the verkle proof.



* use the new db when processing blocks.
  - tree_data:internal_dict_update_trie/2 should work if Trees is a pointer to the verkle root. Just load everything from the Dict as a batch. So it needs to know the height.
    Done, should be tested by mining and syncing some blocks, and confirming that the trees data is replaced with a single pointer, and roots with a single hash.
  - in block:make, we use trees:root_hash. this should be updated to work with a pointer to the verkle tree.
    Done. verify by mining and syncing some blocks.
    
  - proofs:prove(Querys, Trees) should work if the Trees is a pointer to the verkle tree. it should make a verkle proof.
    Done. we need to test that it works somehow.

  - proofs:facts_to_dict needs to work with the verkle proof.
    Write when the test reaches this point. print statement is ready.





* use the new db when making proofs for the api.
  - make a new api that accepts batches of things that we want proved.

* make sure the state root in the header is being calculated reasonably. Look at what block:merkelize is doing to the proofs in the block.


  
* use the new db when generating bocks.
  - block:hash should work with the new format of trees and roots.

* in block_db, instead of storing proofs for every block on a page, store a single proof for all the consensus state you need to verify that entire page.
  - replace the proofs in the blocks with the merkle root of that proof.
  - block hash should work, even if you don't re-create the verkle proof for that block.
  - teach the syncing node how to handle this.


* make sure serializing to json for http works, and that the light node can verify the proofs.

* update the light node to use the new api after the change is activated.

* block access to the old tree after the update.

* set the correct update height.

